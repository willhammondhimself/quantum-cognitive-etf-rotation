{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Analysis\n",
    "\n",
    "Analyze model predictions vs actual returns:\n",
    "- Scatter plots (predictions vs actuals)\n",
    "- Directional accuracy\n",
    "- Error distributions\n",
    "- Ranking agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "with open('../data/processed/processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Load models\n",
    "with open('../outputs/baselines/baseline_models.pkl', 'rb') as f:\n",
    "    baseline_models = pickle.load(f)\n",
    "\n",
    "with open('../outputs/qcml/qcml_models.pkl', 'rb') as f:\n",
    "    qcml_models = pickle.load(f)\n",
    "\n",
    "splits = data['splits']\n",
    "feature_cols = data['feature_cols']\n",
    "\n",
    "# Get test data\n",
    "X_test = splits.test[feature_cols].values\n",
    "y_test = splits.test['excess_return'].values\n",
    "\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Baseline predictions\n",
    "for name, model in baseline_models.items():\n",
    "    if hasattr(model, 'predict'):\n",
    "        predictions[name] = model.predict(X_test)\n",
    "    else:\n",
    "        # PyTorch model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "            predictions[name] = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# QCML predictions\n",
    "for name, model in qcml_models.items():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        predictions[name] = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "print(f\"Models: {list(predictions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions vs Actuals Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test * 100, preds * 100, alpha=0.3, s=10)\n",
    "    \n",
    "    # Reference line\n",
    "    lims = [min(y_test.min(), preds.min()) * 100 - 1, max(y_test.max(), preds.max()) * 100 + 1]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.5, label='Perfect prediction')\n",
    "    \n",
    "    # Compute metrics\n",
    "    corr = np.corrcoef(y_test, preds)[0, 1]\n",
    "    spearman = spearmanr(y_test, preds)[0]\n",
    "    \n",
    "    ax.set_xlabel('Actual Excess Return (%)')\n",
    "    ax.set_ylabel('Predicted Excess Return (%)')\n",
    "    ax.set_title(f'{name}\\nPearson: {corr:.3f}, Spearman: {spearman:.3f}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/backtest/predictions_scatter.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Directional Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute directional accuracy (sign agreement)\n",
    "directional_acc = {}\n",
    "for name, preds in predictions.items():\n",
    "    sign_match = np.sign(preds) == np.sign(y_test)\n",
    "    # Handle zeros\n",
    "    sign_match[y_test == 0] = True\n",
    "    directional_acc[name] = sign_match.mean() * 100\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(directional_acc)))\n",
    "bars = ax.bar(directional_acc.keys(), directional_acc.values(), color=colors)\n",
    "ax.axhline(y=50, color='red', linestyle='--', label='Random (50%)')\n",
    "ax.set_ylabel('Directional Accuracy (%)')\n",
    "ax.set_title('Sign Prediction Accuracy by Model')\n",
    "ax.legend()\n",
    "\n",
    "for bar, val in zip(bars, directional_acc.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{val:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/backtest/directional_accuracy.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    errors = (preds - y_test) * 100  # In percentage points\n",
    "    \n",
    "    ax.hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.axvline(x=errors.mean(), color='green', linestyle='-', label=f'Mean: {errors.mean():.3f}%')\n",
    "    \n",
    "    ax.set_xlabel('Prediction Error (%)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{name}\\nStd: {errors.std():.3f}%, MAE: {np.abs(errors).mean():.3f}%')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/backtest/error_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ranking Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by week and compute ranking correlation\n",
    "test_df = splits.test.copy()\n",
    "test_df['week_idx'] = test_df.groupby('date').ngroup()\n",
    "\n",
    "# Add predictions to dataframe\n",
    "for name, preds in predictions.items():\n",
    "    test_df[f'pred_{name}'] = preds\n",
    "\n",
    "# Compute weekly Spearman correlation\n",
    "weekly_corrs = {name: [] for name in predictions}\n",
    "\n",
    "for week in test_df['week_idx'].unique():\n",
    "    week_data = test_df[test_df['week_idx'] == week]\n",
    "    actuals = week_data['excess_return'].values\n",
    "    \n",
    "    for name in predictions:\n",
    "        preds_week = week_data[f'pred_{name}'].values\n",
    "        if len(actuals) > 2:  # Need at least 3 points for ranking\n",
    "            corr, _ = spearmanr(actuals, preds_week)\n",
    "            if not np.isnan(corr):\n",
    "                weekly_corrs[name].append(corr)\n",
    "\n",
    "# Plot distribution of weekly ranking correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "data_to_plot = [weekly_corrs[name] for name in predictions]\n",
    "bp = ax.boxplot(data_to_plot, labels=list(predictions.keys()), patch_artist=True)\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(predictions)))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Weekly Spearman Correlation')\n",
    "ax.set_title('Distribution of Weekly Ranking Correlations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add mean values\n",
    "means = [np.mean(weekly_corrs[name]) for name in predictions]\n",
    "for i, mean in enumerate(means, 1):\n",
    "    ax.annotate(f'Î¼={mean:.3f}', xy=(i, mean), xytext=(5, 0), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/backtest/ranking_correlations.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance by Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze accuracy by return magnitude\n",
    "abs_returns = np.abs(y_test)\n",
    "quintiles = pd.qcut(abs_returns, 5, labels=['Q1 (small)', 'Q2', 'Q3', 'Q4', 'Q5 (large)'])\n",
    "\n",
    "accuracy_by_quintile = {}\n",
    "for name, preds in predictions.items():\n",
    "    sign_match = np.sign(preds) == np.sign(y_test)\n",
    "    accuracy_by_quintile[name] = pd.Series(sign_match).groupby(quintiles).mean() * 100\n",
    "\n",
    "# Plot\n",
    "acc_df = pd.DataFrame(accuracy_by_quintile)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "acc_df.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.axhline(y=50, color='red', linestyle='--', label='Random')\n",
    "ax.set_xlabel('Return Magnitude Quintile')\n",
    "ax.set_ylabel('Directional Accuracy (%)')\n",
    "ax.set_title('Prediction Accuracy by Return Magnitude')\n",
    "ax.legend(loc='upper right')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/backtest/accuracy_by_magnitude.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile summary statistics\n",
    "summary = []\n",
    "for name, preds in predictions.items():\n",
    "    errors = preds - y_test\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Pearson Corr': f\"{np.corrcoef(y_test, preds)[0,1]:.4f}\",\n",
    "        'Spearman Corr': f\"{spearmanr(y_test, preds)[0]:.4f}\",\n",
    "        'Sign Accuracy': f\"{(np.sign(preds) == np.sign(y_test)).mean()*100:.2f}%\",\n",
    "        'MSE': f\"{np.mean(errors**2):.6f}\",\n",
    "        'MAE': f\"{np.mean(np.abs(errors))*100:.4f}%\",\n",
    "        'Bias': f\"{np.mean(errors)*100:.4f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary).set_index('Model')\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
